from langgraph.graph import StateGraph, END
from langchain_core.messages import BaseMessage
from typing import TypedDict, Annotated, Sequence
import operator
from dotenv import load_dotenv

# ì›Œí¬í”Œë¡œìš° íŒ©í† ë¦¬ import
from workflows.workflow_factory import WorkflowFactory

load_dotenv()

#ê·¸ë˜í”„ ìƒíƒœ(ë©”ëª¨ë¦¬) ì •ì˜
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]
    context: str = ""
    memory: dict = {}
    tool_results: list = []
    should_exit: bool = False
    user_id: str = "default_user"
    last_response: str = ""

def create_single_agent_workflow(agent_core):
    factory = WorkflowFactory(agent_core)

    workflow = StateGraph(AgentState)

    workflow.add_node("user_input", factory.user_input_node_func)
    workflow.add_node("memory", factory.memory_node_func)
    workflow.add_node("rag", factory.rag_node_func)
    workflow.add_node("llm", factory.llm_node_func)
    workflow.add_node("tools", factory.tool_node_func)
    workflow.add_node("mcp_tools", factory.mcp_tool_node_func) # MCP ë„êµ¬ ë…¸ë“œ ì¶”ê°€
    workflow.add_node("output", factory.output_node_func)

    workflow.set_entry_point("user_input")

    # ì‚¬ìš©ì ì…ë ¥ í›„ ì¢…ë£Œ ì—¬ë¶€ í™•ì¸
    workflow.add_conditional_edges(
        "user_input",
        factory.should_exit,
        {
            "exit": END,
            "continue": "memory"
        }
    )

    # ë©”ëª¨ë¦¬ -> RAG -> LLM ìˆœì„œë¡œ ì§„í–‰ (MCP ë„êµ¬ëŠ” LLM ì´í›„ë¡œ ì´ë™)
    workflow.add_edge("memory", "rag")
    workflow.add_edge("rag", "llm")

    # âœ… LLM í›„ ì¡°ê±´ë¶€ ë¶„ê¸°: MCP ë„êµ¬ ìš°ì„  ì‹¤í–‰ (íˆ´ ì½œë§ ì´ìŠˆë¡œ ì¸í•œ ì„ì‹œ ì½”ë“œ)
    def decide_after_llm(state):
        """LLM í›„ ë‹¤ìŒ ë‹¨ê³„ ê²°ì • - ì‹œê°„ ê´€ë ¨ ìš”ì²­ì€ MCP ë„êµ¬ ìš°ì„ """
        messages = state.get("messages", [])
        if not messages:
            return "output"

        # ì›ë³¸ ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ì‹œê°„ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
        user_messages = []
        for msg in messages:
            if hasattr(msg, 'type') and msg.type == 'human':
                user_messages.append(msg)
            elif hasattr(msg, 'content') and not hasattr(msg, 'tool_calls'):
                # HumanMessage í´ë˜ìŠ¤ì¸ ê²½ìš°
                if 'HumanMessage' in str(type(msg)):
                    user_messages.append(msg)

        if user_messages:
            last_user_msg = user_messages[-1]
            content = ""
            if hasattr(last_user_msg, 'content'):
                content = str(last_user_msg.content).lower()

            # ì‹œê°„ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸ (ë” ë„“ì€ ë²”ìœ„)
            time_keywords = [
                'ì‹œê°„', 'í˜„ì¬ì‹œê°„', 'ì§€ê¸ˆì‹œê°„', 'ëª‡ì‹œ', 'ì‹œê°',
                'time', 'current time', 'what time',
                'ë‚ ì§œ', 'ì˜¤ëŠ˜', 'ì§€ê¸ˆ', 'date', 'today', 'now',
                'ì–¸ì œ', 'when'
            ]

            for keyword in time_keywords:
                if keyword in content:
                    print(f"ğŸ•’ ì‹œê°„ ê´€ë ¨ í‚¤ì›Œë“œ '{keyword}' ê°ì§€, MCP ë„êµ¬ ì‹¤í–‰")
                    return "mcp_tools"

        # AI ë©”ì‹œì§€ì— tool_callsê°€ ìˆìœ¼ë©´ ê¸°ì¡´ ë„êµ¬ ì‚¬ìš©
        last_message = messages[-1]
        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
            return "tools"

        return "output"

    workflow.add_conditional_edges(
        "llm",
        decide_after_llm,
        {
            "tools": "tools",
            "mcp_tools": "mcp_tools",
            "output": "output"
        }
    )

    # ë„êµ¬ ì‹¤í–‰ í›„ ë‹¤ì‹œ LLMìœ¼ë¡œ (ê¸°ì¡´ ë„êµ¬)
    workflow.add_edge("tools", "llm")

    # MCP ë„êµ¬ ì‹¤í–‰ í›„ ì¶œë ¥ìœ¼ë¡œ ì§ì ‘ ì´ë™
    workflow.add_edge("mcp_tools", "output")

    # ì¶œë ¥ í›„ ì¢…ë£Œ ì—¬ë¶€ í™•ì¸ (ë£¨í”„ ì œì–´)
    workflow.add_conditional_edges(
        "output",
        factory.should_exit,
        {
            "exit": END,
            "continue": "user_input"
        }
    )

    return workflow.compile()