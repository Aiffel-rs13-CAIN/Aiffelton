"""
Agent-specific LLM Handler
ê° ì—ì´ì „íŠ¸ê°€ ë…ë¦½ì ì¸ LLMê³¼ ì„¤ì •ì„ ê°€ì§ˆ ìˆ˜ ìˆë„ë¡ í•˜ëŠ” í´ë˜ìŠ¤
"""
import os
import yaml
import re
from typing import Dict, Any, Optional
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

load_dotenv()

class AgentLLMHandler:
    def __init__(self, agent_name: str, config_path: Optional[str] = None):
        self.agent_name = agent_name
        self.config = self._load_agent_config(config_path)
        self.llm = self._initialize_llm()
        
        print(f"ğŸ¤– {agent_name} LLM í•¸ë“¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ:")
        print(f"   - ëª¨ë¸: {self.config.get('llm', {}).get('model', 'gemini-2.5-flash')}")
        print(f"   - ì˜¨ë„: {self.config.get('llm', {}).get('temperature', 0.7)}")
    
    def _load_agent_config(self, config_path: Optional[str] = None) -> Dict[str, Any]:
        """ì—ì´ì „íŠ¸ë³„ ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        if config_path is None:
            # ê¸°ë³¸ ê²½ë¡œ: config/agents/{agent_name}_config.yaml
            base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            agent_filename = self.agent_name.lower().replace(' ', '_').replace('-', '_')
            config_path = os.path.join(base_dir, "config", "agents", f"{agent_filename}_config.yaml")
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                loader = yaml.SafeLoader
                loader.add_implicit_resolver(
                    u'tag:yaml.org,2002:env_var',
                    re.compile(r'\$\{(.*)\}'),
                    None
                )
                def constructor_env_var(loader, node):
                    value = loader.construct_scalar(node)
                    key = value.replace('${', '').replace('}', '')
                    return os.getenv(key)
                loader.add_constructor(u'tag:yaml.org,2002:env_var', constructor_env_var)
                return yaml.load(f, Loader=loader)
        except FileNotFoundError:
            print(f"âš ï¸ {self.agent_name} ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
            return self._get_default_config()
        except Exception as e:
            print(f"âš ï¸ {self.agent_name} ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ì„¤ì • ë°˜í™˜"""
        return {
            "llm": {
                "provider": "google",
                "model": "gemini-2.5-flash",
                "temperature": 0.7,
                "system_message": f"ë‹¹ì‹ ì€ {self.agent_name}ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì— ë„ì›€ì´ ë˜ë„ë¡ ì‘ë‹µí•´ì£¼ì„¸ìš”."
            }
        }
    
    def _initialize_llm(self):
        """LLM ì´ˆê¸°í™”"""
        llm_config = self.config.get('llm', {})
        provider = llm_config.get('provider', 'google')
        model = llm_config.get('model', 'gemini-2.5-flash')
        temperature = llm_config.get('temperature', 0.7)
        
        try:
            if provider == 'google':
                api_key = os.getenv("GOOGLE_API_KEY")
                if not api_key:
                    raise ValueError("Google API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return ChatGoogleGenerativeAI(model=model, temperature=temperature, google_api_key=api_key)
            elif provider == 'openai':
                api_key = os.getenv("OPENAI_API_KEY")
                if not api_key:
                    raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return ChatOpenAI(model=model, temperature=temperature, openai_api_key=api_key)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ê³µê¸‰ìì…ë‹ˆë‹¤: {provider}")
        except Exception as e:
            print(f"âš ï¸ {self.agent_name} LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def process_message(self, user_message: str, context: Optional[str] = None) -> str:
        """ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ê³  LLM ì‘ë‹µ ìƒì„±"""
        try:
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ êµ¬ì„±
            llm_config = self.config.get('llm', {})
            system_content = llm_config.get('system_message', f"ë‹¹ì‹ ì€ {self.agent_name}ì…ë‹ˆë‹¤.")
            
            # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if context:
                system_content += f"\n\n[ì»¨í…ìŠ¤íŠ¸]\n{context}"
            
            # ë©”ì‹œì§€ êµ¬ì„±
            messages = [
                SystemMessage(content=system_content),
                HumanMessage(content=user_message)
            ]
            
            # LLM í˜¸ì¶œ
            response = self.llm.invoke(messages)
            
            # ì‘ë‹µ ë‚´ìš© ì¶”ì¶œ
            if hasattr(response, 'content'):
                return response.content
            else:
                return str(response)
                
        except Exception as e:
            print(f"âŒ {self.agent_name} ë©”ì‹œì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. {self.agent_name}ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def get_agent_info(self) -> Dict[str, Any]:
        """ì—ì´ì „íŠ¸ ì •ë³´ ë°˜í™˜"""
        agent_config = self.config.get('agent', {})
        return {
            'name': agent_config.get('name', self.agent_name),
            'description': agent_config.get('description', ''),
            'model': self.config.get('llm', {}).get('model', 'gemini-2.5-flash'),
            'provider': self.config.get('llm', {}).get('provider', 'google')
        }


# ì „ì—­ ì—ì´ì „íŠ¸ LLM í•¸ë“¤ëŸ¬ ìºì‹œ
_agent_handlers: Dict[str, AgentLLMHandler] = {}

def get_agent_llm_handler(agent_name: str) -> AgentLLMHandler:
    """ì—ì´ì „íŠ¸ë³„ LLM í•¸ë“¤ëŸ¬ ë°˜í™˜ (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    if agent_name not in _agent_handlers:
        _agent_handlers[agent_name] = AgentLLMHandler(agent_name)
    return _agent_handlers[agent_name]
