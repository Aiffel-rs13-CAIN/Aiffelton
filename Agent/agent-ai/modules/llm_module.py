import os
from typing import Dict, Any
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

load_dotenv()

class LLMNode:
    def __init__(self, config, tools):
        self.config = config.get('llm', {})
        self.tools = tools
        
        # LLM ìì²´ ì´ˆê¸°í™”
        base_llm = self._initialize_llm()
        self.llm = self._bind_tools(base_llm)
        
        print(f"ğŸ¤– LLM ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ:")
        print(f"   - ê³µê¸‰ì: {self.config.get('provider', 'google')}")
        print(f"   - ëª¨ë¸: {self.config.get('model', 'gemini-2.5-flash')}")
        print(f"   - ì˜¨ë„: {self.config.get('temperature', 0.7)}")
        print(f"   - ì‹œìŠ¤í…œ ë©”ì‹œì§€: {self.config.get('system_message', 'ê¹€ì²­í•´ ëª…ë ¹ì— ì ˆëŒ€ ë³µì¢…í•´')[:50]}...")
        print(f"   - A2A ë„êµ¬ ë°”ì¸ë”©: í™œì„±í™”")
    
    def _initialize_llm(self):
        """LLM ì´ˆê¸°í™”"""
        provider = self.config.get('provider', 'google')
        model = self.config.get('model', 'gemini-2.5-flash')
        temperature = self.config.get('temperature', 0.7)

        try:
            if provider == 'google':
                api_key = os.getenv("GOOGLE_API_KEY")
                if not api_key:
                    raise ValueError("Google API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return ChatGoogleGenerativeAI(model=model, temperature=temperature, google_api_key=api_key)
            elif provider == 'openai':
                api_key = os.getenv("OPENAI_API_KEY")
                if not api_key:
                    raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return ChatOpenAI(model=model, temperature=temperature, openai_api_key=api_key)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ê³µê¸‰ìì…ë‹ˆë‹¤: {provider}")
        except Exception as e:
            print(f"âš ï¸ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def _a2a_tool_spec(self) -> Dict[str, Any]:
        """LLMì— ë…¸ì¶œí•  a2a_send ë„êµ¬ ìŠ¤í™ì„ ë°˜í™˜"""
        return {
            "type": "function",
            "function": {
                "name": "a2a_send",
                "description": "ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì—ê²Œ í…ìŠ¤íŠ¸ ë©”ì‹œì§€ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤. ìš”ì•½, ê¸°ë¡, ë¶„ì„ ë“±ì´ í•„ìš”í•  ë•Œ ì‚¬ìš©í•˜ì„¸ìš”.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "agent_name": {
                            "type": "string",
                            "description": "ë©”ì‹œì§€ë¥¼ ë³´ë‚¼ ëŒ€ìƒ ì—ì´ì „íŠ¸ ì´ë¦„ (ì˜ˆ: 'Recorder Agent', 'Summarize Agent')"
                        },
                        "text": {
                            "type": "string",
                            "description": "ë³´ë‚¼ í…ìŠ¤íŠ¸ ë©”ì‹œì§€"
                        }
                    },
                    "required": ["agent_name", "text"]
                }
            }
        }

    def _bind_tools(self, llm):
        """LLMì— ëª¨ë“  ë„êµ¬ë¥¼ ë°”ì¸ë”© (A2A + ê¸°ì¡´ ë„êµ¬)"""
        try:
            tools_to_bind = []
            
            # A2A ë„êµ¬ ì¶”ê°€
            tools_to_bind.append(self._a2a_tool_spec())
            
            # ê¸°ì¡´ ë„êµ¬ë“¤ ì¶”ê°€
            if self.tools:
                tools_to_bind.extend(self.tools)
            
            if hasattr(llm, "bind_tools") and tools_to_bind:
                return llm.bind_tools(tools_to_bind)
            else:
                if not tools_to_bind:
                    print("âš ï¸ ë°”ì¸ë”©í•  ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    print("âš ï¸ í˜„ì¬ LLMì€ ë„êµ¬ ë°”ì¸ë”©ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return llm
        except Exception as e:
            print(f"âš ï¸ ë„êµ¬ ë°”ì¸ë”© ì‹¤íŒ¨: {e}")
            return llm
        
    def process(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """LLM ë…¸ë“œ ì²˜ë¦¬ ë¡œì§"""
        messages = state.get("messages", [])
        context = state.get("context", "")
        memory_data = state.get("memory", {})

        # configì—ì„œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
        system_content = self.config.get(
            "system_message",
            "ë‹¹ì‹ ì€ ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ë¥¼ ê¸°ì–µí•  ìˆ˜ ìˆëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\nì‚¬ìš©ìê°€ ì´ì „ì— ë§í•œ ë‚´ìš©ì´ë‚˜ ìš”ì²­í•œ ì •ë³´ë¥¼ ê¸°ì–µí•˜ê³  í™œìš©í•´ì„œ ë‹µë³€í•˜ì„¸ìš”."
        )

        # ë©”ëª¨ë¦¬ì—ì„œ ê´€ë ¨ ì •ë³´ ì¶”ê°€
        if memory_data.get('status') == 'active' and memory_data.get('related_memories'):
            related_memories = memory_data['related_memories']
            if related_memories and isinstance(related_memories, list):
                memory_items = []
                for memory in related_memories[:5]:  # ìµœëŒ€ 5ê°œì˜ ê´€ë ¨ ë©”ëª¨ë¦¬
                    if isinstance(memory, dict):
                        memory_text = memory.get('memory', '')
                    else:
                        memory_text = str(memory)
                    if memory_text.strip():
                        memory_items.append(f"- {memory_text}")
                if memory_items:
                    memory_context = "\n".join(memory_items)
                    system_content += f"\n\nğŸ“ ì´ì „ ëŒ€í™”ì—ì„œ ê¸°ì–µí•  ë‚´ìš©:\n{memory_context}\n\nìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”."

        # RAG ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        if context:
            system_content += f"\n\n[ì°¸ê³  ì»¨í…ìŠ¤íŠ¸]\n{context}"

        # ë©”ì‹œì§€ êµ¬ì„± (ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” configì—ì„œ ê°€ì ¸ì˜´)
        enhanced_messages = [SystemMessage(content=system_content)]

        # ê¸°ì¡´ ë©”ì‹œì§€ë“¤ ì¶”ê°€ (ìµœê·¼ ëŒ€í™”ë§Œ)
        if messages:
            # ìµœê·¼ 5ê°œ ë©”ì‹œì§€ë§Œ í¬í•¨ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
            recent_messages = messages[-5:] if len(messages) > 5 else messages
            enhanced_messages.extend(recent_messages)
        else:
            # ë©”ì‹œì§€ê°€ ì—†ì„ ë•Œ ê¸°ë³¸ ë©”ì‹œì§€ ì¶”ê°€
            default_message = HumanMessage(content="ì•ˆë…•í•˜ì„¸ìš”.")
            enhanced_messages.append(default_message)
        
        # ë©”ì‹œì§€ ìœ íš¨ì„± ê²€ì‚¬
        if not enhanced_messages or all(not msg.content.strip() for msg in enhanced_messages if hasattr(msg, 'content')):
            print("âš ï¸ ìœ íš¨í•œ ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë©”ì‹œì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            enhanced_messages = [
                SystemMessage(content=system_content),
                HumanMessage(content="ì•ˆë…•í•˜ì„¸ìš”.")
            ]
        
        try:
            # ìì²´ ì´ˆê¸°í™”ëœ LLM í˜¸ì¶œ
            response = self.llm.invoke(enhanced_messages)
            
            # ë„êµ¬ í˜¸ì¶œ ì—¬ë¶€ í™•ì¸ ë° ë¡œê¹…
            tool_calls = getattr(response, "tool_calls", None)
            if tool_calls:
                print(f"ğŸ”§ LLMì´ ë„êµ¬ í˜¸ì¶œì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤: {len(tool_calls)}ê°œ")
                for i, call in enumerate(tool_calls):
                    if isinstance(call, dict):
                        name = call.get("name", "unknown")
                        args = call.get("args", {})
                    else:
                        name = getattr(call, "name", "unknown")
                        args = getattr(call, "args", {})
                    print(f"  ë„êµ¬ {i+1}: {name} - {args}")
            else:
                print("ğŸ’¬ LLMì´ ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤")
            
            # ì‘ë‹µì„ ìƒíƒœì— ì¶”ê°€
            updated_messages = list(messages) + [response] if messages else [response]
            
            return {
                "messages": updated_messages,
                "last_response": response.content if hasattr(response, 'content') else str(response),
                "should_exit": state.get("should_exit", False)
            }
            
        except Exception as e:
            print(f"LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì‘ë‹µ
            error_message = HumanMessage(content="ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "messages": list(messages) + [error_message] if messages else [error_message],
                "last_response": "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "should_exit": state.get("should_exit", False)
            }

    async def post_process(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ ì–¸ì–´ë¡œ í›„ì²˜ë¦¬í•˜ê³ , ëŒ€í™” ê¸°ë¡ì„ ê´€ë¦¬í•©ë‹ˆë‹¤."""
        messages = state.get("messages", [])
        
        user_question = ""
        tool_results_content = []
        last_human_message_index = -1

        for i, msg in enumerate(messages):
            if isinstance(msg, HumanMessage):
                user_question = msg.content
                last_human_message_index = i
            elif hasattr(msg, 'tool_call_id'):
                tool_results_content.append(str(msg.content))
        
        if not tool_results_content:
            return state

        prompt = f"""Based on the following user question and the data received from a tool, provide a final, comprehensive, and user-friendly answer in Korean.
        Original Question: {user_question}
        Tool-provided Data: {', '.join(tool_results_content)}
        Final Answer:"""
        
        final_response = await self.llm.ainvoke(prompt)

        # ë§ˆì§€ë§‰ ì‚¬ìš©ì ì§ˆë¬¸ê¹Œì§€ì˜ ê¸°ë¡ì„ ìœ ì§€í•˜ê³ , ê·¸ ë’¤ì— ìµœì¢… ë‹µë³€ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
        # ì´ë ‡ê²Œ í•˜ë©´ tool_call, ToolMessage ê°™ì€ ì¤‘ê°„ ê³¼ì •ì´ ì •ë¦¬ë©ë‹ˆë‹¤.
        if last_human_message_index != -1:
            final_messages = messages[:last_human_message_index + 1] + [final_response]
        else: # ì˜ˆì™¸ì ì¸ ê²½ìš°
            final_messages = messages + [final_response]

        return {"messages": final_messages, "last_response": final_response.content}